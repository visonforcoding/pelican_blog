Title: ç¨‹åºå‘˜ä»£ç ä¸‹çš„è®¸è±ªæ°(ä¸‹ æŠ€æœ¯ç¯‡)
Date: 2017-08-09 23:19
Category: å…¶ä»–
>æ¥ä¸Šç¯‡ï¼Œè¿™ä¸€ç¯‡å°†ä»æŠ€æœ¯å±‚é¢è®²è®²æ˜¯å¦‚ä½•å®ç°çš„ã€‚é˜…è¯»æœ¬æ–‡æ‚¨å°†ä¼šäº†è§£å¦‚ä½•ç”¨pythonçˆ¬å–å¾®åšçš„è¯„è®ºä»¥åŠå¦‚ä½•ç”¨python word_cloudåº“è¿›è¡Œæ•°æ®å¯è§†åŒ–ã€‚

ä¸Šä¸€ç¯‡:[ç¨‹åºå‘˜ä»£ç ä¸‹çš„è®¸è±ªæ°](http://www.jianshu.com/p/ab49113c1fda)

## å‡†å¤‡å·¥ä½œ

æ‰“å¼€å¾®åšpc mç«™å¹¶æ‰¾åˆ°è®¸è±ªæ°è¯¥æ¡å¾®åšåœ°å€:https://m.weibo.cn/status/4132385564040383

**ä¸ºä»€ä¹ˆè¦ç”¨mç«™åœ°å€ï¼Ÿå› ä¸ºmç«™å¯ä»¥ç›´æ¥æŠ“å–åˆ°api jsonæ•°æ®,è€Œpcç«™è™½ç„¶ä¹Ÿæœ‰apiè¿”å›çš„æ˜¯html,ç›¸æ¯”è€Œè¨€é€‰å–mç«™ä¼šçœå»å¾ˆå¤šéº»çƒ¦**

æ‰“å¼€è¯¥é¡µé¢ï¼Œå¹¶ä¸”ç”¨chrome çš„æ£€æŸ¥å·¥å…· æŸ¥çœ‹networkï¼Œå¯ä»¥è·å–åˆ°è¯„è®ºçš„apiåœ°å€ã€‚

![chromeæŸ¥çœ‹](http://upload-images.jianshu.io/upload_images/4033700-382e5a613595bc1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## æ•°æ®æŠ“å–

é¦–å…ˆè§‚å¯Ÿapiè¿”å›

![image.png](http://upload-images.jianshu.io/upload_images/4033700-f2aa3ddad169fe31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

ä»è¿”å›åœ°å€ä¸Šå¯ä»¥çœ‹åˆ°å¯ä»¥é€šè¿‡å‚æ•°page æ”¹å˜è¯·æ±‚çš„é¡µç ,å¹¶ä¸”æ¯é¡µéƒ½å›è¿”å›æ€»æ¡æ•°å’Œæ€»é¡µç æ•°ã€‚è¿™é‡Œæˆ‘å†³å®šé‡‡ç”¨**å¤šçº¿ç¨‹æ¥æŠ“å»**(å…¶å®æ•°æ®é‡ä¸å¤§,ä¹Ÿå¯ä»¥å•çº¿ç¨‹è·‘)ã€‚

### å…¶ä¸­åœ¨çˆ¬å–æ•°æ®çš„æ—¶å€™ä¼šé¢ä¸´å‡ ä¸ªé—®é¢˜ï¼š

**1.å­˜å‚¨é€‰æ‹©**

æˆ‘è¿™é‡Œé€‰ç”¨äº†MongoDBä½œä¸ºæ•°æ®å­˜å‚¨ï¼Œå› ä¸ºapié€šå¸¸è¿”å›çš„æ˜¯jsonæ•°æ®è€Œjsonç»“æ„å’ŒMongoDBçš„å­˜å‚¨æ–¹å¼å¯ä»¥ç»“åˆçš„å¾ˆé»˜å¥‘ï¼Œä¸éœ€è¦ç»è¿‡ä»»ä½•å¤„ç†å¯ä»¥ç›´æ¥çš„è¿›è¡Œæ’å…¥ã€‚

**2.é˜²çˆ¬è™«**

å¾ˆå¤šç½‘ç«™å¯èƒ½ä¼šåšä¸€äº›é˜²çˆ¬è™«çš„å¤„ç†ï¼Œé¢å¯¹åŒä¸€ä¸ªè¯·æ±‚ipçš„çŸ­æ—¶é—´çš„é«˜é¢‘ç‡è¯·æ±‚ä¼šè¿›è¡ŒæœåŠ¡éš”æ–­(ç›´æ¥å‘Šè¯‰ä½ æœåŠ¡ä¸å¯ç”¨)ï¼Œè¿™ä¸ªæ—¶å€™å¯ä»¥å»ç½‘ä¸Šæ‰¾ä¸€äº›ä»£ç†è¿›è¡Œè¯·æ±‚ã€‚

**3.å¤šçº¿ç¨‹çš„ä»»åŠ¡åˆ†é…**

é‡‡ç”¨å¤šçº¿ç¨‹çˆ¬å–ä½ å½“ç„¶ä¸èƒ½è®©å¤šä¸ªçº¿ç¨‹å»çˆ¬å–åŒæ ·çš„é“¾æ¥åšåˆ«äººå·²ç»åšè¿‡çš„äº‹æƒ…ï¼Œé‚£æ ·å¤šçº¿ç¨‹æ¯«æ— æ„ä¹‰ã€‚æ‰€ä»¥ä½ éœ€è¦åˆ¶å®šä¸€å¥—è§„åˆ™ï¼Œè®©ä¸åŒçº¿ç¨‹çˆ¬å–ä¸åŒçš„é“¾æ¥ã€‚

```python
# coding=utf-8
from __future__ import division
from pymongo import MongoClient
import requests
import sys
import re
import random
import time
import logging
import threading
import json
from os import path
import math

# çˆ¬å–å¾®åšè¯„è®º
# mç«™å¾®åšåœ°å€
weibo_url = 'https://m.weibo.cn/status/4132385564040383' 

thread_nums = 5  #çº¿ç¨‹æ•°

#ä»£ç†åœ°å€
proxies = {
  "http": "http://171.92.4.67:9000",
  "http": "http://163.125.222.240:8118",
  "http": "http://121.232.145.251:9000",
  "http": "http://121.232.147.247:9000",
  
}


# åˆ›å»º æ—¥å¿— å¯¹è±¡
logger = logging.getLogger()
handler = logging.StreamHandler()
formatter = logging.Formatter(
    '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.DEBUG)

mongoconn = MongoClient('127.0.0.1', 27017)
mdb = mongoconn.data_analysis
das_collection = mdb.weibo

weiboid_reobj = re.match(r'.*status/(\d+)', weibo_url)
weibo_id = weiboid_reobj.group(1)


def scrapy_comments(weibo_id, page):
    weibo_comment_url = 'https://m.weibo.cn/api/comments/show?id=%s&page=%d' % (
        weibo_id, page)
    res = requests.get(weibo_comment_url)
    res_obj = json.loads(res.content)
    return res_obj


def import_comments(threadName, weibo_id, page_start, page_end):
    logger.info('å¼€å§‹çº¿ç¨‹:%s' % threadName)
    for page in range(page_start, page_end + 1):
        logging.info('è¯»å–ç¬¬%sé¡µ' % page)
        time.sleep(1)
        # continue
        try:
            res_obj = scrapy_comments(weibo_id, page)
            logging.info('è¯¥é¡µæœ‰%sæ¡è®°å½•' % len(res_obj['data']))
        except:
            logging.error('è¯»å–%sé¡µæ—¶å‘ç”Ÿé”™è¯¯' % page)
            continue
        if res_obj['ok'] == 1:
            comments = res_obj['data']
            for comment in comments:
                comment_text = re.sub(
                    r'</?\w+[^>]*>', '', comment['text']).encode('utf-8')
                if re.search(r'å›å¤@.*:', comment_text):
                    # è¿‡æ»¤æ‰å›å¤åˆ«äººçš„è¯„è®º
                    continue
                comment['text'] = comment_text
                comment['weibo_id'] = weibo_id
                logging.info('è¯»å–è¯„è®º:%s' % comment['id'])
                try:
                    if das_collection.find_one({'id': comment['id']}):
                        logging.info('åœ¨mongodbä¸­å­˜åœ¨')
                    else:
                        logging.info('æ’å…¥è®°å½•:%s' % comment['id'])
                        das_collection.insert_one(comment)
                except:
                    logging.error('mongodbå‘ç”Ÿé”™è¯¯')
        else:
            logging.error('è¯»å–ç¬¬%sé¡µæ—¶å‘ç”Ÿé”™è¯¯' % page)
    logging.info('çº¿ç¨‹%sç»“æŸ' % threadName)
    # res_obj = scrapy_comments(weibo_id, page)


if __name__ == '__main__':
    # åˆ†é…ä¸åŒé“¾æ¥åˆ°ä¸åŒçš„çº¿ç¨‹ä¸Šå»
    res_obj = scrapy_comments(weibo_id, 1)
    if res_obj['ok'] == 1:
        total_number = res_obj['total_number']
        logging.info('è¯¥æ¡å¾®åšæœ‰:%sæ¡è¯„è®º' % total_number)
        max_page = res_obj['max']
        page_nums = math.ceil(max_page / thread_nums)
    else:
        raise

    # print max_page
    # print page_nums

    for i in range(1, thread_nums + 1):
        if i < thread_nums:
            page_end = page_nums * i
        else:
            page_end = max_page
        page_start = (i - 1) * page_nums + 1

        t = threading.Thread(target=import_comments, args=(
            i, weibo_id, int(page_start), int(page_end)))
        t.start()

```
## æ•°æ®æ•´ç†å¯è§†åŒ–(data visualization)

è¿è¡Œè„šæœ¬å®Œæ¯•ï¼Œæˆ‘çš„MongoDBå¾—åˆ°äº†2ä¸‡å¤šæ¡è¯„è®ºæ•°æ®ï¼Œæ¥ä¸‹æ¥è¦åšçš„äº‹æ˜¯å¯¹è¿™éƒ¨åˆ†æ•°æ®è¿›è¡Œæå–ã€æ¸…æ´—ã€ç»“æ„åŒ–ç­‰æ“ä½œã€‚è¿™é‡Œé¡ºä¾¿è¯´æ˜ä¸€ä¸‹python æ•°æ®åˆ†æçš„ å¤§è‡´åŸºæœ¬æµç¨‹ã€‚

**1.ä¸å¤–ç•Œè¿›è¡Œäº¤äº’**
è¿™ä¸ªè¿‡ç¨‹åŒ…æ‹¬æ•°æ®çš„è·å–ã€è¯»å–ã€‚ä¸ç®¡æ˜¯ä»ç½‘ç»œèµ„æºä¸Šçˆ¬å–ã€è¿˜æ˜¯ä»ç°æœ‰èµ„æº(å„æ ·çš„æ–‡ä»¶å¦‚æ–‡æœ¬ã€excelã€æ•°æ®åº“å­˜å‚¨å¯¹è±¡)

**2.å‡†å¤‡å·¥ä½œ**
å¯¹æ•°æ®è¿›è¡Œæ¸…æ´—(cleaning)ã€ä¿®æ•´(munging)ã€æ•´åˆ(combining)ã€è§„èŒƒåŒ–(normalizing)ã€é‡å¡‘(reshaping)ã€åˆ‡ç‰‡(slicing)å’Œåˆ‡å—(dicing)

**3.è½¬æ¢**
å¯¹æ•°æ®é›†åšä¸€äº›æ•°å­¦å’Œç»Ÿè®¡è¿ç®—äº§ç”Ÿæ–°çš„æ•°æ®é›†

**4.å»ºæ¨¡å’Œè®¡ç®—**
å°†æ•°æ®è·Ÿç»Ÿè®¡æ¨¡å‹ã€æœºå™¨å­¦ä¹ ç®—æ³•æˆ–å…¶ä»–è®¡ç®—å·¥å…·è”ç³»èµ·æ¥

**5.å±•ç¤º**
åˆ›å»ºäº¤äº’å¼çš„æˆ–é™æ€çš„å›¾ç‰‡æˆ–æ–‡å­—æ‘˜è¦

ä¸‹é¢æˆ‘ä»¬æ¥è¿›è¡Œ2ã€3åŠ5çš„å·¥ä½œ:

```python
# coding=utf-8
import sys
from pymongo import MongoClient
import random
# åˆ†è¯åº“
# from snownlp import SnowNLP
import jieba
import uniout
from collections import Counter, OrderedDict
# è¯è¯­äº‘ æ–‡æœ¬ç»Ÿè®¡å¯è§†åŒ–åº“
from wordcloud import WordCloud


mongoconn = MongoClient('127.0.0.1', 27017)
mdb = mongoconn.data_analysis
das_collection = mdb.weibo


total_counts = das_collection.find().count()

# random_int = random.randint(0, total_counts - 1)
docs = das_collection.find()
print docs.count()
words_counts = {}
for doc in docs:
    print doc
    comment_text = doc['text'].encode('utf-8')
    if len(comment_text) == 0:
        continue
    words = jieba.cut(comment_text)
    for word in words:
        if word not in words_counts:
            words_counts[word] = 1
        else:
            words_counts[word] += 1

for word in words_counts.keys():
    if words_counts[word] < 2 or len(word) < 2:
        del words_counts[word]

# print words_counts.items()
#æ³¨æ„è¦è®©ä¸­æ–‡ä¸ä¹±ç è¦æŒ‡å®šä¸­æ–‡å­—ä½“
#fit_words æ¥æ”¶å‚æ•°æ˜¯dict  eg:{'ä½ ':333,'å¥½':23}  æ–‡å­—:å‡ºç°æ¬¡æ•°
wordcloud = WordCloud(
    font_path='/Users/cwp/font/msyh.ttf',
    background_color='white',
    width=1200,
    height=1000
).fit_words(words_counts)
import matplotlib.pyplot as plt
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
```
ä»‹ç»ä¸‹ä»¥ä¸Šä»£ç ï¼š
æˆ‘ä»¬ä¸»è¦ç”¨åˆ°äº†2ä¸ªå·¥å…·ï¼Œjiebaå’Œword_cloudã€‚å‰è€…å¯¹ä¸­æ–‡è¿›è¡Œåˆ†è¯åè€…å›¾å½¢åŒ–å±•ç¤ºè¯è¯­çš„å‡ºç°é¢‘ç‡ã€‚
ä¼—æ‰€å‘¨çŸ¥ï¼Œä¸­æ–‡ç³»çš„è¯­è¨€å¤„ç†ææ€•æ˜¯æœ€éš¾çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)çš„è¯­ç§ã€‚å°±åŸºæœ¬çš„åˆ†è¯è€Œè¨€éƒ½æ˜¯ä¸€é¡¹æ¯”è¾ƒå›°éš¾çš„å·¥ä½œ,(è‹±è¯­å¥å­ä¸­æ¯ä¸ªå•è¯éƒ½æ˜¯æœ‰ç©ºæ ¼åˆ†å¼€çš„ï¼Œè€Œä¸­æ–‡æ˜¯ç”±å•ä¸ªå­—ç»„æˆè¯è¿æ¥æˆä¸²ç»„æˆå¥).
ä¸¾ä¸ªä¾‹å­,è¯·ç”¨â€œå­©æâ€é€ å¥,"é‚£ä¸ªç”·å­©æäº¤å®Œä»£ç å°±ä¸‹ç­äº†"ã€‚å¦‚æœäººå·¥åˆ†è¯ï¼Œå¯ä»¥çŸ¥é“"ç”·å­©"å’Œ"æäº¤"åº”è¯¥æ˜¯åˆ†å¼€çš„2ä¸ªè¯ï¼Œ**ä½†æ˜¯å¯¹äºæœºå™¨è€Œè¨€ï¼Œè¦è¾¨åˆ«"æ"åº”è¯¥ä¸"ç”·"è¿˜æ˜¯"äº¤"è¿›è¡Œç»„è¯å°±å¾ˆéš¾åŠäº†**ã€‚è¦æƒ³æœºå™¨èƒ½å¤Ÿæ›´ç²¾ç¡®çš„è¾¨åˆ«è¿™ç±»é—®é¢˜ï¼Œå°±éœ€è¦è®©æœºå™¨ä¸åœå­¦ä¹ ï¼Œè®©å®ƒçŸ¥é“è¿™ç§æƒ…å†µè¯¥è¿™ä¹ˆåˆ†è€Œä¸æ˜¯é‚£ä¹ˆåˆ†ã€‚ç ”ç©¶ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å°†æ˜¯ä¸€ä¸ªé•¿ä¹…è€Œå¤§çš„å·¥ç¨‹ï¼Œå¯¹äºåˆ†ææ•°æ®(æˆ‘ä»¬ä¸æ˜¯è¦ç ”ç©¶è‡ªç„¶è¯­è¨€å¤„ç†ğŸ˜)ï¼Œè¿™é‡Œå°±å€ŸåŠ©jiebaè¿™ä¸ªåº“è¿›è¡Œå·¥ä½œäº†.

å¯¹äºword_cloud,å›¾å½¢åŒ–æ–‡æœ¬ç»Ÿè®¡ï¼Œç½‘ä¸Šæœ‰ä¸å°‘çš„åšæ–‡éƒ½è´´äº†ä»£ç ï¼Œä½†æˆ‘æƒ³è¯´çš„æ˜¯æˆ‘ä¸äº†è§£å®ƒä»¬æ˜¯ä¸æ˜¯çœŸçš„è¿è¡Œå‡ºäº†ç»“æœã€‚å› ä¸ºfit_words è¿™ä¸ªå‡½æ•°æ¥æ”¶çš„æ˜¯dictè€Œä¸æ˜¯listï¼Œå®˜æ–¹æ–‡æ¡£å’Œå‡½æ•°docå…¶å®å†™é”™äº†,åœ¨githubä¸Šæœ‰æŠ«éœ²ã€‚

æœ€åå¾—åˆ°ç»“æœ:

![ç»“æœ](http://upload-images.jianshu.io/upload_images/4033700-dfbee241840ea23e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## ä¸€äº›ç”¨åˆ°çš„å·¥å…·

1.[word_cloud A little word cloud generator in Python](https://github.com/amueller/word_cloud)

2.[jieba ç»“å·´ä¸­æ–‡åˆ†è¯](https://github.com/fxsjy/jieba)

3.[Requests is the only Non-GMO HTTP library for Python, safe for human consumption.](http://docs.python-requests.org/en/master/)