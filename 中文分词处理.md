Title: 中文分词处理
Date: 2018-03-16 10:42:30
Category: 方案
keywords: 中文分词 NLP 自然语言处理

> 中文分词是中文NLP的第一步，也是一个难题 大课题，本文旨在收集目前开源的方案和做一些对比


## thulac

THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：

- 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。

- 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。

- 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。

- 目前支持python、java、c++ 版本，提供训练模型(v1_v2简单模型和需填写资料审核的复杂模型)

官网地址:[http://thulac.thunlp.org/](http://thulac.thunlp.org/)


